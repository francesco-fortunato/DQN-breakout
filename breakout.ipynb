{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87094bd",
   "metadata": {},
   "source": [
    "# Breakout: A performance comparison between a FeedForward and a Convolutional Neural Network\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"./recordings/ATARI_Breakout_Eval_model_21700_reward_357.gif\" style=\"width:30%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "    <img src=\"./pictures/Screenshot 2024-02-26 193156.png\" style=\"width:30%; height:auto; margin-right:20px;\">\n",
    "    <img src=\"./pictures/Screenshot 2024-02-26 193417.png\" style=\"width:10%; height:auto;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd533502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from breakout_wrapper import make_atari_breakout, wrap\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import signal\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2dcffe",
   "metadata": {},
   "source": [
    "## Feed-forward Neural Network\n",
    "\n",
    "- Input Layer: The model takes an input of shape (84, 84, 4), representing a stack of four 84x84 grayscale frames. This allows the model to consider temporal information over a sequence of frames.\n",
    "\n",
    "- Flatten Layer: The input is flattened into a one-dimensional vector to be processed by fully connected layers.\n",
    "\n",
    "- Dense Layers: Two dense (fully connected) layers with 64 units each and ReLU activation functions are applied successively.\n",
    "\n",
    "- Output Layer: The final dense layer produces an output vector with a length equal to the number of actions (4 in our case). We used a linear activation function to output Q-values for each action, representing the expected future rewards for taking each action from the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce5862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 4\n",
    "def create_q_model():\n",
    "   \n",
    "   inputs = layers.Input(shape=(84, 84, 4))\n",
    "\n",
    "   flattened = layers.Flatten()(inputs) \n",
    "\n",
    "   dense1 = layers.Dense(64, activation=\"relu\")(flattened)\n",
    "   dense2 = layers.Dense(64, activation=\"relu\")(dense1)\n",
    "   output_layer = layers.Dense(num_actions, activation=\"linear\")(dense2)\n",
    "\n",
    "   return keras.Model(inputs=inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f0120",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "The network architecture is designed based on the Deepmind paper and specifically\n",
    "tailored for training on Atari 2600. Convolutional layers are used to capture\n",
    "spatial dependencies and patterns in the game frames. This is crucial for Atari\n",
    "Breakout because it involves complex visual information, and convolutional layers\n",
    "are effective in learning hierarchical features.\n",
    "\n",
    "A dense neural layer, also known as a fully connected layer, didn't work as \n",
    "effectively for processing Atari Breakout frames as expected (we will see this)\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "![](./pictures/Network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e25302",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 4\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 4,))\n",
    "\n",
    "    # Define the first convolutional layer\n",
    "    # - 32 filters, each 8x8 in size\n",
    "    # - Stride of 4, meaning the filter moves 4 pixels at a time\n",
    "    # - ReLU activation function is applied to the output\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "\n",
    "    # Define the second convolutional layer\n",
    "    # - 64 filters, each 4x4 in size\n",
    "    # - Stride of 2\n",
    "    # - ReLU activation function\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "\n",
    "    # Define the third convolutional layer\n",
    "    # - 64 filters, each 3x3 in size\n",
    "    # - Stride of 1\n",
    "    # - ReLU activation function\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    # Flatten the output from the convolutional layers\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    # Define a fully connected layer with 512 neurons\n",
    "    # - ReLU activation function\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "\n",
    "    # Output layer with num_actions neurons (4 in this case for the Breakout game)\n",
    "    # - Linear activation function\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ac8d5",
   "metadata": {},
   "source": [
    "\n",
    "# Setup\n",
    "\n",
    "### Exploration-exploitation trade-off\n",
    "\n",
    "As we know, in reinforcement learning, agents face the dilemma of whether to explore new actions or exploit known ones to maximize rewards. The exploration-exploitation trade-off is crucial for balancing between discovering potentially better actions and exploiting known optimal ones.\n",
    "\n",
    "$\\epsilon$ is a function of the number of frames the agent has seen. For the first 50000 frames the agent only explores ($\\epsilon=1$). Over the following 1 million frames, $\\epsilon$ is linearly decreased to 0.1, meaning that the agent starts exploiting more and more. DeepMind then keeps $\\epsilon=0.1$, however, we chose to decrease it to $\\epsilon=0.01$ over the remaining frames (24kk) as suggested by the [OpenAi Baselines for DQN](https://openai.com/research/openai-baselines-dqn) (in the plot the maximum number of frames is 2 million for demonstration purposes).\n",
    "\n",
    "![](./pictures/epsilon.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02996e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_final = 0.01  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "epsilon_interval_2 = (\n",
    "    epsilon_min - epsilon_final\n",
    ")  # Rate at which to reduce chance of random action being taken after 1kk frames\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000.0   # Number of frames with epsilon set to 1.0\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0 # Number of frames to linearly decay epsilon from 1 to 0.1\n",
    "epsilon_final_frames = 24000000.0 # Number of frames to linearly decay epsilon from 0.1 to 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d56aff",
   "metadata": {},
   "source": [
    "## Atari wrappers\n",
    "\n",
    "### NoopResetEnv \n",
    "This wrapper adds a random number of “no-op” (no-operation) actions to the start of each episode to introduce some randomness and make the agent explore more.\n",
    "### FireResetEnv \n",
    "This wrapper automatically presses the “FIRE” button at the start of each episode, which is required for some Atari games to start.\n",
    "### EpisodicLifeEnv\n",
    "This wrapper resets the environment whenever the agent loses a life, rather than when the game is over, to make the agent learn to survive for longer periods.\n",
    "### MaxAndSkipEnv\n",
    "This wrapper skips a fixed number of frames (usually 4) and returns the maximum pixel value from the skipped frames, to reduce the impact of visual artifacts and make the agent learn to track moving objects.\n",
    "### ClipRewardEnv\n",
    "This wrapper clips the reward signal to be either -1, 0, or 1, to make the agent focus on the long-term goal of winning the game rather than short-term rewards.\n",
    "### WarpFrame\n",
    "This wrapper resizes and converts the game screen frames to grayscale to reduce the input size and make it easier for the agent to learn.\n",
    "We modify this code to make it even more suitable just for breakout. In particular, instead of resizing the image from 210x160 to 84x84, we first crop the image to make it 160x160 (we remove the upper part which represents the actual score and remainings lifes), and then we apply the resizing.\n",
    "\n",
    "Original Image:\n",
    "![](./pictures/frame_00_delay-0.02s.png)\n",
    "\n",
    "Cropped Image:\n",
    "![](./pictures/cropped.png) Then resizing, finally;\n",
    "\n",
    "Greyscale Image: \n",
    "![](./pictures/grey.png)\n",
    "\n",
    "### ScaledFloatFrame\n",
    "This wrapper scales the pixel values to be between 0 and 1 to make the input data more compatible with deep learning models. It's a sort of \"brightness normalization\"\n",
    "### make_atari\n",
    "This function creates an Atari environment with various settings suitable for deep reinforcement learning research, including the use of the NoFrameskip wrapper and a maximum number of steps per episode.\n",
    "### wrap_deepmind\n",
    "This function applies a combination of the defined wrappers to the given env object, including EpisodicLifeEnv, FireResetEnv, WarpFrame, ClipRewardEnv, and FrameStack. The scale argument can be used to include the ScaledFloatFrame wrapper as well.\n",
    "### FrameStack\n",
    "This wrapper stacks a fixed number of frames together to give the agent some temporal information and make it easier for the agent to learn the dynamics of the game.\n",
    "\n",
    "Given this image, can you tell where the ball is going? \n",
    "\n",
    "![](./pictures/frame_00_delay-0.02s.png)\n",
    "\n",
    "Of course no. What about this sequence instead?\n",
    "\n",
    "![](./pictures/frame_00_delay-0.02s.png) ![](./pictures/frame_01_delay-0.02s.png) ![](./pictures/frame_02_delay-0.02s.png) ![](./pictures/frame_03_delay-0.02s.png)\n",
    "\n",
    "\n",
    "We will see the results at the end...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Baseline Atari environment because of Deepmind helper functions\n",
    "env = make_atari_breakout(\"BreakoutNoFrameskip-v4\")\n",
    "# Warp the frames, grey scale, stake four frame and scale to smaller ratio\n",
    "env = wrap(env, frame_stack=True, scale=True)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a3059",
   "metadata": {},
   "source": [
    "### Replay memory and other parameters\n",
    "\n",
    "Experiences are stored in a replay buffer and the model is periodically trained using sampled batches from the buffer. We have given a maximum dimension for the memory like it is done in the Deepmind paper, but a different number since it we less computation power. Implementing a replay buffer can help the training process because experiences are often highly correlated temporally therefore using consecutive experiences directly for training can lead to instability and slow learning. Without a replay buffer, an agent might overfit to recent experiences and fail to generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38574458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "batch_size = 32  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 10000 \n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "terminal_life_lost = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad52b3",
   "metadata": {},
   "source": [
    "## Huber Loss\n",
    "\n",
    "One other interesting thing to notice: DeepMind uses the quadratic cost function with error clipping (see page 7 of [Mnih et al. 2015](https://www.nature.com/articles/nature14236/)).\n",
    "\n",
    ">We also found it helpful to clip the error term from the update [...] to be between -1 and 1. Because the absolute value loss function |x| has a derivative of -1 for all negative values of x and a derivative of 1 for all positive values of x, clipping the squared error to be between -1 and 1 corresponds to using an absolute value loss function for errors outside of the (-1,1) interval. This form of error clipping further improved the stability of the algorithm.\n",
    "\n",
    "Why does this improve the stability of the algorithm?\n",
    "\n",
    ">In deep networks or recurrent neural networks, error gradients can accumulate during an update and result in very large gradients. These in turn result in large updates to the network weights, and in turn, an unstable network. At an extreme, the values of weights can become so large as to overflow and result in NaN values. [Source](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)\n",
    "\n",
    "This so-called exploding gradient problem can, to some extent, be avoided by clipping the gradients to a certain threshold value, if they exceed it: * If the true gradient is larger than a critical value $x$, just assume it is $x$.* Observe that the derivate of the green curve does not increase (or decrease) for $x>1$ (or $x<-1$).\n",
    "Error clipping can be easily implemented in tensorflow by using the Huber loss function `tf.losses.huber_loss`.\n",
    "\n",
    "![](pictures/huber.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e35b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
    "# improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first model makes the predictions for Q-values which are used to\n",
    "# make a action.\n",
    "model = create_q_model()\n",
    "# Build a target model for the prediction of future rewards.\n",
    "# The weights of a target model get updated every 10000 steps thus when the\n",
    "# loss between the Q-values is calculated the target Q-value is stable.\n",
    "model_target = create_q_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec682bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"training_stats.csv\"\n",
    "# Check if the CSV file exists\n",
    "if os.path.exists(csv_filename):\n",
    "    with open(csv_filename, mode='r') as file:\n",
    "        # CSV file already exists, read the header\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "else:\n",
    "    # CSV file does not exist, create and write the header\n",
    "    header = [\"Episode\", \"Total Reward\", \"Epsilon\", \"Avg Reward (Last 100)\", \"Total Frames\",\n",
    "              \"Frame Rate\", \"Model Updates\", \"Running Reward\", \"Training Time\"]\n",
    "    with open(csv_filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63c0f2",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee3e40",
   "metadata": {},
   "source": [
    "Every time step, the agent chooses an action based on the epsilon, takes a step in the environment, stores this transition, then takes a random batch of 32 transitions and uses them to train the neural network. For every training item (s, a, r, s`) in the mini batch of 32 transitions, the network is given a state (stack of 4 frames, or s). Using the next state and the Bellman equation we get the targets for our neural network. Basically if the next state is a terminal state, meaning the episode has ended, then the target is equal to just the immediate reward. Otherwise, the state action pair should map to the value of the immediate reward, plus the discount multiplied by the value of next state’s highest value action. We can achieve this thanks to the done_sample array. Traditionally, the value of the next state’s highest value action is obtained by running the next state through the neural network we’re trying to train. But this can lead to oscillations and divergence of the policy. So instead, we use a target network which helps mitigate oscillations and divergence. The target network’s weights are updated to the weights of the training network every 10 000 time steps.\n",
    "\n",
    "![](./pictures/bellman.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c3983-d938-4a5f-a5c2-d2feedbeda90",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "starting = datetime.datetime.now()\n",
    "while True:  # Run until solved\n",
    "    start_time = time.time()\n",
    "    state = np.array(env.reset())\n",
    "\n",
    "    current_lives = 5\n",
    "    \n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take the best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        if frame_count > epsilon_random_frames: # Decay epsilon only after exploring for first 50k frames\n",
    "            if epsilon > epsilon_min:\n",
    "                # Decay probability of taking random action\n",
    "                epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "                epsilon = max(epsilon, epsilon_min)\n",
    "            else:\n",
    "                # Continue decaying epsilon linearly over the remaining frames\n",
    "                epsilon -= epsilon_interval_2 / (epsilon_final_frames)\n",
    "                epsilon = max(epsilon, epsilon_final)\n",
    "\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "            \n",
    "        episode_reward += reward\n",
    "\n",
    "        # When a life is lost, we save terminal_life_lost = True in the replay memory\n",
    "        # N.B. We don't modify directly done, since done is already used to break the loop\n",
    "        num_lives = info['lives']\n",
    "\n",
    "        if (num_lives < current_lives):\n",
    "            terminal_life_lost = True\n",
    "            current_lives = num_lives\n",
    "        else:\n",
    "            terminal_life_lost = False\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(terminal_life_lost if not done else done) # If the game is not terminated, if life lost add true, else add done (False or true)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            ) # turns True into 1.0 and False into 0.0.\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            # updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "            #    future_rewards, axis=1\n",
    "            # )\n",
    "\n",
    "            # Our Implementation\n",
    "            # If the game is over because the agent lost or won, there is no next state and the value is simply the reward \n",
    "\n",
    "            updated_q_values = rewards_sample + (1 - done_sample) * gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values (If action taken was 1, it create [0,1,0,0])\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                #  to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            # print(info)\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    # Calculate additional statistics\n",
    "    avg_reward_last_100 = np.mean(episode_reward_history[-100:])\n",
    "    frame_rate = frame_count / (time.time() - start_time)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # Append the episode statistics to the CSV file\n",
    "    with open(csv_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([episode_count, episode_reward, epsilon, avg_reward_last_100,\n",
    "                            frame_count, frame_rate, len(done_history),\n",
    "                            running_reward, training_time])\n",
    "    \n",
    "    if (episode_count%100 == 0):\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "        print(f\"{current_time} - Episode {episode_count} reached. Saving model in saved_models/model_episode_{episode_count}. . .\")\n",
    "        model.save(\"saved_models/model_episode_{}\".format(episode_count))\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"{current_time} - Model saved.\")\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"{current_time} - Saving target model. . .\")\n",
    "        # Save the target model\n",
    "        model_target.save(\"saved_models/target_model_episode_{}\".format(episode_count))\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"{current_time} - Target model saved in saved_models/target_model_episode_{episode_count}.\")\n",
    "\n",
    "    episode_count += 1\n",
    "    if (num_lives==0):\n",
    "        template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
    "        print(template.format(running_reward, episode_count, frame_count))\n",
    "\n",
    "    if running_reward > 40:  # 40 is the avg score of human beings\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        episode_count -= 1\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be45104",
   "metadata": {},
   "source": [
    "# Evaluation and Performance Comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb648ef3",
   "metadata": {},
   "source": [
    "## Training Comparison\n",
    "\n",
    "### Training Process: Dense vs Convolutional\n",
    "\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "    <img src=\"./reward_plot_training_feed.png\" style=\"width:50%; height:auto; margin-right:20px;\">\n",
    "    <img src=\"./reward_plot_training.png\" style=\"width:50%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "### Some interesting insight: Using terminal life lost\n",
    "\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "    <img src=\"./reward_plot_wothout life_loss_terminal.png\" style=\"width:50%; height:auto; margin-right:20px;\">\n",
    "    <img src=\"./reward_plot_with_life_loss_terminal.png\" style=\"width:50%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "### Why stacking frames?\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "    <img src=\"./reward_plot_dual.png\" style=\"width:50%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e017cb9",
   "metadata": {},
   "source": [
    "# Evaluation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import pandas as pd\n",
    "from breakout_wrapper import make_atari_breakout, wrap\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Configuration parameters\n",
    "seed = 42\n",
    "num_actions = 4\n",
    "\n",
    "# Use the Baseline Atari environment for testing\n",
    "env = make_atari_breakout(\"BreakoutNoFrameskip-v4\")\n",
    "# Warp the frames, grey scale, stack four frames, and scale to a smaller ratio\n",
    "env = wrap(env, frame_stack=True, scale=True, clip_rewards=False, episode_life=False)\n",
    "env.seed(seed)\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "episode_count = 0\n",
    "max_episodes = 10  # Set the desired number of episodes\n",
    "\n",
    "# Load training stats CSV\n",
    "training_stats_file = \"training_stats.csv\"  # Replace with the actual filename\n",
    "training_stats_df = pd.read_csv(training_stats_file)\n",
    "\n",
    "# Path to the saved model\n",
    "model_filename = \"saved_models/model_episode_{}\".format(episode_count)\n",
    "\n",
    "# Create the absolute path to the model file\n",
    "absolute_model_path = os.path.join(current_directory, model_filename)\n",
    "\n",
    "# Load the pre-trained model\n",
    "loaded_model = tf.keras.models.load_model(absolute_model_path)\n",
    "\n",
    "# Function to generate GIF from raw frames\n",
    "def generate_gif(frame_number, frames_for_gif, reward, path, ep):\n",
    "    imageio.mimsave(f'{path}{\"ATARI_Breakout_Eval_model_{0}_reward_{1}.gif\".format(ep, int(reward))}',\n",
    "                    frames_for_gif, duration=1/30)\n",
    "    print(f'Gif saved at {path}{\"ATARI_Breakout_Eval_model_{0}_reward_{1}.gif\".format(ep, int(reward))}')\n",
    "\n",
    "# Function to choose an action based on the model's predictions with epsilon-greedy exploration\n",
    "def choose_action(model, state):\n",
    "    # Exploit: choose the action with the highest predicted value\n",
    "    state_tensor = tf.convert_to_tensor(state)\n",
    "    state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "    action_probs = model(state_tensor, training=False)\n",
    "    # Take the best action\n",
    "    action = tf.argmax(action_probs[0]).numpy()\n",
    "    return action\n",
    "\n",
    "\n",
    "for episode_count in range(0, training_stats_df['Episode'].max() + 1, 100):\n",
    "    print(f'Testing episode {episode_count}. . .')\n",
    "    # Path to the saved model\n",
    "    model_filename = \"saved_models/model_episode_{}\".format(episode_count)\n",
    "\n",
    "    # Create the absolute path to the model file\n",
    "    absolute_model_path = os.path.join(current_directory, model_filename)\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    loaded_model = tf.keras.models.load_model(absolute_model_path)\n",
    "\n",
    "    # Initialize variables\n",
    "    highest_reward = float('-inf')  # Variable to track the highest reward\n",
    "    frames_highest_reward = []  # List to store frames associated with the highest reward\n",
    "\n",
    "    # Test the model in the environment\n",
    "    frames_for_gif = []  # List to store raw RGB frames for GIF generation\n",
    "    current_lives = 5\n",
    "    restart = True\n",
    "    episode_reward = 0\n",
    "    episode_counter = 0\n",
    "\n",
    "    # Lists to store Q values during the episode\n",
    "    q_values = []\n",
    "\n",
    "    # Lists to store rewards for calculating the average over 10 episodes\n",
    "    rewards_for_average = []\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    while episode_counter < max_episodes:\n",
    "        frame_count += 1\n",
    "        if restart:\n",
    "            restart = False\n",
    "            state = np.array(env.reset())\n",
    "            state_next, reward, done, info = env.step(1)  # Play Fire Action\n",
    "            state = np.array(state_next)\n",
    "\n",
    "        # Capture the raw RGB frame for GIF generation\n",
    "        raw_frame = env.render(mode='rgb_array')\n",
    "        frames_for_gif.append(raw_frame)\n",
    "\n",
    "        # Comment this and uncomment the other to play yourself\n",
    "        action = choose_action(loaded_model, state)\n",
    "\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        state = np.array(state_next)\n",
    "\n",
    "        num_lives = info['lives']\n",
    "        print(info)\n",
    "        print(done)\n",
    "        print(num_lives)\n",
    "\n",
    "        # Get the Q value for the chosen action\n",
    "        q_value = np.max(loaded_model.predict(np.expand_dims(state, axis=0)))\n",
    "        q_values.append(q_value)\n",
    "\n",
    "        if num_lives < current_lives:\n",
    "            state_next, reward, done, info = env.step(1)\n",
    "            state = np.array(state_next)\n",
    "            current_lives = num_lives\n",
    "            print(num_lives)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode n \" + str(episode_counter))\n",
    "            # Store the maximum reward and frames with the highest rewards\n",
    "            if episode_reward > highest_reward:\n",
    "                highest_reward = episode_reward\n",
    "                frames_highest_reward = frames_for_gif.copy()\n",
    "\n",
    "            # Print the average of Q values\n",
    "            avg_q_value = np.mean(q_values)\n",
    "            print(f\"Average Q Value: {avg_q_value}\")\n",
    "\n",
    "            # Store rewards for calculating the average over 10 episodes\n",
    "            rewards_for_average.append(episode_reward)\n",
    "\n",
    "            # Get the corresponding frame count from training stats\n",
    "            episode_row = training_stats_df[training_stats_df['Episode'] == episode_count]\n",
    "            if not episode_row.empty:\n",
    "                frame_count_training = episode_row['Total Frames'].values[0]\n",
    "                # Store episode_count, frame_count, avg_reward, max_reward, avg_q_value in your evaluation CSV\n",
    "\n",
    "            # Reset variables for the next episode\n",
    "            frames_for_gif = []\n",
    "            episode_reward = 0\n",
    "            state = np.array(env.reset())\n",
    "            env.step(1)\n",
    "            current_lives = 5\n",
    "            episode_reward = 0\n",
    "            restart = True\n",
    "            episode_counter += 1\n",
    "\n",
    "    # Generate GIF for the episode with the highest reward\n",
    "    generate_gif(0, frames_highest_reward, highest_reward, 'recordings/', episode_count)\n",
    "\n",
    "    # Print the average reward over 10 episodes\n",
    "    avg_reward_over_10_episodes = np.mean(rewards_for_average)    \n",
    "    print(\"Individual Rewards for Each Episode:\", rewards_for_average)\n",
    "    print(f\"Average Reward Over 10 Episodes: {avg_reward_over_10_episodes}\")\n",
    "    # Store episode_count, frame_count, avg_reward, max_reward, avg_q_value in your evaluation CSV\n",
    "    evaluation_data = {\n",
    "        'episode_count': episode_count,\n",
    "        'frame_count_training': frame_count_training,\n",
    "        'avg_reward': avg_reward_over_10_episodes,\n",
    "        'max_reward': highest_reward,\n",
    "        'avg_q_value': avg_q_value\n",
    "    }\n",
    "    # Append the evaluation data to your CSV file\n",
    "    with open('evaluation_stats.csv', 'a') as f:\n",
    "        pd.DataFrame([evaluation_data]).to_csv(f, header=f.tell()==0, index=False)  # Append without header if the file is not empty\n",
    "\n",
    "    print(f'Testing episode {episode_count} finished. . .')\n",
    "    del loaded_model\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2c8ab0",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "\n",
    "## Reward (score)\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "    <img src=\"./reward_plot_evaluation_reward_feed.png\" style=\"width:50%; height:auto; margin-right:20px;\">\n",
    "    <img src=\"./reward_plot_evaluation_reward.png\" style=\"width:50%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "## Q-values\n",
    "<div style=\"display:flex; justify-content:center;\">\n",
    "    <img src=\"./reward_plot_evaluation_q_value_feed.png\" style=\"width:50%; height:auto; margin-right:20px;\">\n",
    "    <img src=\"./reward_plot_evaluation_q_values.png\" style=\"width:50%; height:auto;\">\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (gymenv)",
   "language": "python",
   "name": "gymenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
